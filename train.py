import mxnet as mx
import argparse
import logging
import os
import utils
import config


def run_train(logger):
    """
    The training process heavily depends on doing fine-tuning the learning rate (config.LEARNING_RATE) 
    to meet the desired result (train/val accuracy & train/val loss)

    This function performs the network training from pre-trained VGG16 dataset.
    """
    # construct the training image iterator
    train_iter = mx.io.ImageRecordIter(
        path_imgrec=config.TRAIN_REC_FILE,
        data_shape=config.IMAGE_SIZE,
        batch_size=config.BATCH_SIZE,
        rand_crop=True,
        rand_mirror=True,
        rotate=15,
        max_shear_ratio=0.1,
        mean_r=config.R_MEAN,
        mean_g=config.G_MEAN,
        mean_b=config.B_MEAN,
        preprocess_threads=config.NUM_DEVICES * 2)

    # construct the validation image iterator
    val_iter = mx.io.ImageRecordIter(
        path_imgrec=config.VAL_REC_FILE,
        data_shape=config.IMAGE_SIZE,
        batch_size=config.BATCH_SIZE,
        mean_r=config.R_MEAN,
        mean_g=config.G_MEAN,
        mean_b=config.B_MEAN)

    # initialize the optimizer SGD
    opt = mx.optimizer.SGD(learning_rate=config.LEARNING_RATE,
                           momentum=0.9, wd=0.0005, rescale_grad=1.0 / config.BATCH_SIZE)
    ctx = [mx.gpu(0)]

    # initialize the model argument and auxiliary parameters
    arg_params = None
    aux_params = None
    allow_missing = False

    # For the first epoch which starting to traing from pre-training vggnet,
    # we only interest in performing the training for the final fc layer
    if start_epoch <= 0:
        logger.debug("Load the pre-training VGG16")
        (symbol, arg_params, aux_params) = mx.model.load_checkpoint(
            "vgg16/vgg16", 0)
        allow_missing = True

        logger.debug("Drop out layers prior FC8 layer")
        layers = symbol.get_internals()
        net = layers["drop7_output"]

        logger.debug(
            f"Initialize the new FC layer with {config.NUM_CLASSES} car classes (make & model)")
        # Only take last FC layers for fine-tuning, other layers are frezzed
        net = mx.sym.FullyConnected(
            data=net, num_hidden=config.NUM_CLASSES, name="fc8")
        net = mx.sym.SoftmaxOutput(data=net, name="softmax")
        arg_params = dict({k: arg_params[k]
                           for k in arg_params if "fc8" not in k})
    else:
        # Load information stored from previous checkpoint
        logger.debug(
            f"Load the model argument and auxiliary parameters from checkpoint#{start_epoch}")
        (net, arg_params, aux_params) = mx.model.load_checkpoint(
            config.CHECKPOINT_PATH, start_epoch)

    # The callback is needed to define the frequent output of result generated by mxnet
    batch_end_cb = [mx.callback.Speedometer(config.BATCH_SIZE, 50)]
    epoch_end_cb = [mx.callback.do_checkpoint(config.CHECKPOINT_PATH)]
    metrics = [mx.metric.Accuracy(), mx.metric.TopKAccuracy(top_k=5),
               mx.metric.CrossEntropy()]

    logger.debug("Network training started...")
    model = mx.mod.Module(symbol=net, context=ctx)
    model.fit(
        train_iter,
        eval_data=val_iter,
        num_epoch=config.MAX_EPOCH,
        begin_epoch=start_epoch,
        initializer=mx.initializer.Xavier(),
        arg_params=arg_params,
        aux_params=aux_params,
        optimizer=opt,
        allow_missing=allow_missing,
        eval_metric=metrics,
        batch_end_callback=batch_end_cb,
        epoch_end_callback=epoch_end_cb
    )


if __name__ == "__main__":
    # Before running this, do revise following parameters in config.py
    # 1. LEARNING_RATE: the regularization technique to obtain the better accuracy and less over fitting
    # 2. MAX_EPOCH: the maximum number of epoches for the traing. Remember each epoch will output >500MB model output.
    # 3. MODEL_PROCESS_CONTEXT: specify the array of hardware (CPU, GPU) used to traing the model. The training duration will be shorted with GPU. MXNET support mutiple GPUS, the
    # it can put list of your GPU into this araay.
    # Example: python train.py -e

    # Init the argument parser
    ap = argparse.ArgumentParser()
    ap.add_argument("-s", "--start_epoch", type=int, default=0,
                    help="resume the training from this input epoch")
    args = vars(ap.parse_args())

    # Specify the epoch number to start/resume the training from last checkpoint
    start_epoch = args["start_epoch"]

    # Init logger
    logger = utils.get_logger(f"./logs/training_{start_epoch}.log")

    try:
        run_train(logger)
    except Exception as ex:
        error_message = utils.exception_to_string(ex)
        logger.error(error_message)
